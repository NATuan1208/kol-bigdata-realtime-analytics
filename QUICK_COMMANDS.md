# Quick Command Reference ‚Äî KOL Platform (Shared Infrastructure)

## üöÄ Daily Workflow

### 1. Start Everything
```bash
# Prerequisite: Ensure SME Pulse is running first
# Then:
cd /path/to/kol-platform
make check-sme    # Verify SME Pulse is ready
make up-kol       # Start KOL services
```

### 2. Check Status
```bash
make ps-kol       # Show KOL services status
docker ps         # Show all running containers
```

### 3. View Logs
```bash
make logs-kol         # All KOL services
make logs-api         # API only
make logs-trainer     # Trainer only
make logs-spark       # Spark only
make logs-redpanda    # Redpanda only
```

### 4. Stop Services
```bash
make down-kol     # Stop KOL (SME Pulse keeps running)
```

---

## üï∑Ô∏è TikTok Scraper - Parallel Workers (Recommended)

### üöÄ One-Click Start: Full Platform
```powershell
# Start to√†n b·ªô platform (Spark Streaming + Scrapers + Metrics Refresh)
.\scripts\start_full_platform.ps1

# V·ªõi options
.\scripts\start_full_platform.ps1 -NoComments -NoProducts -MaxKols 10
```

Script s·∫Ω:
1. ‚úÖ Check infrastructure (Redis, Kafka, Spark)
2. ‚úÖ Install redis trong Spark containers
3. ‚úÖ Start Spark Streaming job ‚Üí Redis
4. ‚úÖ Start Discovery Daemon (t√¨m KOL m·ªõi)
5. ‚úÖ Start Video/Comment/Product Workers (delay 240s)
6. ‚úÖ Start Metrics Refresh (t√≠nh velocity)

### üîÑ Kh·ªüi ch·∫°y 5 Workers Song Song
```powershell
# Kh·ªüi ch·∫°y workers:
# - Discovery Scraper (Default profile) - T√¨m KOL m·ªõi ‚Üí push kol.discovery.raw
# - Metrics Refresh - Re-push tracked KOLs ƒë·ªÉ t√≠nh velocity
# - Video Stats Worker (Profile 1) - L·∫•y profile + videos
# - Comment Extractor (Profile 1) - Extract comments
# - Product Extractor (Profile 6) - Extract products
.\scripts\start_parallel_scrapers.ps1

# Ch·ªâ l·∫•y products (kh√¥ng l·∫•y comments)
.\scripts\start_parallel_scrapers.ps1 -NoComments

# Custom settings v·ªõi 2 intervals ri√™ng bi·ªát
.\scripts\start_parallel_scrapers.ps1 -MaxKols 10 -MaxVideos 30 `
    -DiscoveryInterval 7200 `  # 2 ti·∫øng t√¨m KOL m·ªõi
    -RefreshInterval 300       # 5 ph√∫t t√≠nh velocity

# Kh√¥ng ch·∫°y Metrics Refresh
.\scripts\start_parallel_scrapers.ps1 -NoRefresh
```

### Ch·∫°y t·ª´ng worker ri√™ng (5 terminals)
```powershell
# Terminal 1: Discovery scraper (t√¨m username m·ªõi) - ch·∫°y m·ªói 2 ti·∫øng
py -m ingestion.sources.kol_scraper daemon --discovery-only --interval 7200

# Terminal 2: Metrics Refresh (t√≠nh velocity) - ch·∫°y m·ªói 5 ph√∫t
py -m ingestion.sources.metrics_refresh --interval 300

# Terminal 3: Video Stats Worker (l·∫•y profile + video stats) - delay 240s
py -m ingestion.consumers.video_stats_worker --max-videos 20 --start-delay 240

# Terminal 4: Comment Extractor (l·∫•y comments) - delay 240s
py -m ingestion.consumers.comment_extractor --max-comments 50 --start-delay 240

# Terminal 5: Product Extractor (l·∫•y products t·ª´ TikTok Shop) - delay 240s
py -m ingestion.consumers.product_extractor --max-videos 20 --start-delay 240
```

### ‚öôÔ∏è Intervals v√† Delays
| Parameter | Default | M√¥ t·∫£ |
|-----------|---------|-------|
| `DiscoveryInterval` | 7200s (2h) | T·∫ßn su·∫•t t√¨m KOL m·ªõi |
| `RefreshInterval` | 300s (5min) | T·∫ßn su·∫•t re-push ƒë·ªÉ t√≠nh velocity |
| `WorkerDelay` | 240s (4min) | Workers ƒë·ª£i Discovery push messages |

### ‚ö†Ô∏è Reset Kafka Consumer Offset (n·∫øu b·ªã ƒë·ªçc l·∫°i message c≈©)
```powershell
# Xem offset hi·ªán t·∫°i
docker exec -it kol-redpanda rpk group describe kol-video-stats-v3
docker exec -it kol-redpanda rpk group describe kol-comment-extractor-v3
docker exec -it kol-redpanda rpk group describe kol-product-extractor-v3

# Reset v·ªÅ LATEST (ch·ªâ ƒë·ªçc message m·ªõi)
docker exec -it kol-redpanda rpk group seek kol-video-stats-v3 --to end
docker exec -it kol-redpanda rpk group seek kol-comment-extractor-v3 --to end
docker exec -it kol-redpanda rpk group seek kol-product-extractor-v3 --to end
```

---

## üìä Spark ETL Jobs

### Load Kafka ‚Üí MinIO (Iceberg)
```powershell
# Ch·∫°y batch job load data t·ª´ Kafka v√†o MinIO
docker exec kol-spark-master /opt/spark/bin/spark-submit `
    --master spark://spark-master:7077 `
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,org.apache.hadoop:hadoop-aws:3.3.4 `
    --conf spark.hadoop.fs.s3a.endpoint=http://sme-minio:9000 `
    --conf spark.hadoop.fs.s3a.access.key=minio `
    --conf spark.hadoop.fs.s3a.secret.key=minio123 `
    --conf spark.hadoop.fs.s3a.path.style.access=true `
    /opt/spark/work-dir/streaming/spark_jobs/kafka_to_iceberg_simple.py --mode batch
```

### Product Tracker (Track sold_count changes)
```powershell
# Dry-run ƒë·ªÉ xem products c·∫ßn track
python -m batch.feature_store.product_tracker --local --dry-run

# Ch·∫°y th·∫≠t (scrape sold_count m·ªõi t·ª´ TikTok Shop)
python -m batch.feature_store.product_tracker --local

# Ch·∫°y headless (kh√¥ng hi·ªán browser)
python -m batch.feature_store.product_tracker --local --headless

# Ch·∫°y trong Docker cluster
docker exec -it kol-spark-master spark-submit `
    --master spark://spark-master:7077 `
    /opt/spark/work-dir/batch/feature_store/product_tracker.py
```

---

## üï∑Ô∏è TikTok Scraper - Single Mode (Legacy)

### Ch·∫°y nhanh (all defaults)
```powershell
# Daemon mode - ch·∫°y li√™n t·ª•c (discovery + profile + videos)
.\.venv\Scripts\python.exe -m ingestion.sources.kol_scraper daemon

# Ch·ªâ discovery (d√πng v·ªõi parallel workers)
.\.venv\Scripts\python.exe -m ingestion.sources.kol_scraper daemon --discovery-only

# Ch·∫°y 1 l·∫ßn t·ª´ng mode
.\.venv\Scripts\python.exe -m ingestion.sources.kol_scraper discovery
.\.venv\Scripts\python.exe -m ingestion.sources.kol_scraper full --max-kols 5
```

### Ch·∫°y v·ªõi custom options
```powershell
# Custom interval (m·∫∑c ƒë·ªãnh 300s = 5 ph√∫t)
.\.venv\Scripts\python.exe -m ingestion.sources.kol_scraper daemon --interval 600

# Gi·ªõi h·∫°n s·ªë KOL m·ªói v√≤ng
.\.venv\Scripts\python.exe -m ingestion.sources.kol_scraper daemon --max-kols-per-round 10

# Full options
.\.venv\Scripts\python.exe -m ingestion.sources.kol_scraper daemon `
    --interval 300 `
    --max-kols-per-round 20 `
    --discovery-only `
    --kafka-broker localhost:19092
```

---

## üîß Network Management

### Create Shared Network (Run Once)
```bash
make network-create
# OR manually:
docker network create sme-network
```

### Inspect Network
```bash
make network-inspect
# Shows all containers connected to sme-network
```

---

## üè• Health Checks

### Verify SME Pulse Prerequisites
```bash
make check-sme
```
**Checks:**
- ‚úÖ sme-network exists
- ‚úÖ sme-postgres is running
- ‚úÖ sme-minio is running
- ‚úÖ sme-trino is running
- ‚úÖ sme-hive-metastore is running

### Check Service Health
```bash
# MLflow
curl http://localhost:5000/health

# API
curl http://localhost:8080/healthz

# Redpanda
docker exec kol-redpanda rpk cluster health

# Cassandra
docker exec kol-cassandra cqlsh -e "DESCRIBE CLUSTER"

# Redis
docker exec kol-redis redis-cli ping
```

---

## üîÑ Restart Services

### Restart All KOL
```bash
make restart-kol
```

### Restart Individual Service
```bash
docker compose -f infra/docker-compose.kol.yml restart <service>
# Examples:
docker compose -f infra/docker-compose.kol.yml restart api
docker compose -f infra/docker-compose.kol.yml restart mlflow
docker compose -f infra/docker-compose.kol.yml restart spark-master
```

---

## üêö Access Containers

### API Container
```bash
docker exec -it kol-api bash
```

### Trainer Container
```bash
docker exec -it kol-trainer bash
```

### Spark Master
```bash
docker exec -it kol-spark-master bash
```

### Cassandra
```bash
docker exec -it kol-cassandra cqlsh
```

### Redis
```bash
docker exec -it kol-redis redis-cli
```

---

## üö® Trending Stream (Realtime hot scores)

This job computes per-KOL velocities and a TrendingScore and writes
results into Redis for dashboards. It is separate from the Iceberg
ingest stream.

### Prerequisites Setup (One-time)
```powershell
# Install redis client inside Spark containers
docker exec -u root kol-spark-master pip install redis
docker exec -u root infra-spark-worker-1 pip install redis
docker exec -u root infra-spark-worker-2 pip install redis

# Verify installation
docker exec kol-spark-master python3 -c "import redis; print('redis OK')"

# Fix Ivy cache permission (n·∫øu g·∫∑p permission error)
docker exec -u root kol-spark-master mkdir -p /home/spark/.ivy2
docker exec -u root kol-spark-master chmod -R 777 /home/spark/.ivy2

# Clear old checkpoints (n·∫øu thay ƒë·ªïi schema)
docker exec kol-spark-master rm -rf /tmp/kafka-trending-checkpoint
```

### Run Trending Stream Job
```powershell
# Submit trending job (background)
docker exec -d kol-spark-master /opt/spark/bin/spark-submit `
    --master spark://spark-master:7077 `
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 `
    /opt/spark-jobs/kafka_trending_stream.py
```

### Trending Formula
```
score = ALPHA * view_velocity + BETA * like_velocity + GAMMA * share_velocity

# Defaults:
ALPHA = 1.0   # View weight
BETA = 0.5    # Like weight  
GAMMA = 0.2   # Share weight

# Velocity = current - previous (delta gi·ªØa 2 l·∫ßn scrape)
```

### Check Redis Trending Data
```powershell
# List all tracked KOLs
docker exec kol-redis redis-cli KEYS "streaming_scores:*"

# Get score for specific KOL
docker exec kol-redis redis-cli HGETALL "streaming_scores:xuannhilamgido"

# Sample output:
#  1) "ts"           ‚Üí timestamp
#  2) "score"        ‚Üí trending score
#  3) "view"         ‚Üí total views
#  4) "like"         ‚Üí total likes
#  5) "share"        ‚Üí total shares
#  6) "video_count"  ‚Üí number of videos
#  7) "view_vel"     ‚Üí view velocity
#  8) "like_vel"     ‚Üí like velocity
#  9) "share_vel"    ‚Üí share velocity
```

### Environment variables (optional):
- `REDIS_HOST`, `REDIS_PORT`, `TREND_ALPHA`, `TREND_BETA`, `TREND_GAMMA`


---

## üìä Access Web UIs

| Service | URL | Credentials |
|---------|-----|-------------|
| MLflow UI | http://localhost:5000 | No auth |
| Inference API | http://localhost:8080 | Token in .env.kol |
| Spark Master UI | http://localhost:8084 | No auth |
| Redpanda Console | http://localhost:8082 | No auth |
| Jupyter Notebook | http://localhost:8888 | Token in .env.kol |
| MinIO Console (SME) | http://localhost:9001 | minioadmin/minioadmin |
| Trino UI (SME) | http://localhost:8080 | No auth |
| Airflow (SME) | http://localhost:8081 | admin/admin |

---

## üîç Debugging

### Check Network Connectivity
```bash
# From trainer container
docker exec -it kol-trainer bash
ping sme-postgres
ping sme-minio
ping sme-trino
```

### Test Database Connection
```bash
docker exec -it kol-trainer python -c "
import psycopg2
conn = psycopg2.connect(
    host='sme-postgres',
    port=5432,
    database='mlflow',
    user='admin',
    password='admin'
)
print('‚úì Connected to sme-postgres')
conn.close()
"
```

### Test MinIO Connection
```bash
docker exec -it kol-trainer python -c "
import boto3
s3 = boto3.client(
    's3',
    endpoint_url='http://sme-minio:9000',
    aws_access_key_id='minioadmin',
    aws_secret_access_key='minioadmin'
)
print('‚úì Connected to sme-minio')
print('Buckets:', [b['Name'] for b in s3.list_buckets()['Buckets']])
"
```

### View Container Resource Usage
```bash
docker stats
# Shows CPU, memory, network I/O for all containers
```

---

## üóÑÔ∏è Database Operations

### Connect to PostgreSQL (SME)
```bash
docker exec -it sme-postgres psql -U admin -d mlflow
```

### Create MLflow Database
```sql
CREATE DATABASE mlflow;
GRANT ALL PRIVILEGES ON DATABASE mlflow TO admin;
```

### List Databases
```bash
docker exec -it sme-postgres psql -U admin -c "\l"
```

---

## üì¶ MinIO Operations

### List Buckets
```bash
docker exec sme-minio mc alias set local http://localhost:9000 minioadmin minioadmin
docker exec sme-minio mc ls local/
```

### Create MLflow Bucket
```bash
docker exec sme-minio mc mb local/mlflow
```

### Check Bucket Size
```bash
docker exec sme-minio mc du local/mlflow
```

---

## ‚òÅÔ∏è Kafka/Redpanda Operations

### Create Topics (Ch·∫°y 1 l·∫ßn khi setup)
```bash
# T·∫°o t·∫•t c·∫£ KOL topics
docker exec kol-redpanda rpk topic create kol.discovery.raw --partitions 4
docker exec kol-redpanda rpk topic create kol.profiles.raw --partitions 4
docker exec kol-redpanda rpk topic create kol.videos.raw --partitions 4
docker exec kol-redpanda rpk topic create kol.comments.raw --partitions 4
docker exec kol-redpanda rpk topic create kol.products.raw --partitions 4
docker exec kol-redpanda rpk topic create kol.scraper.events --partitions 1

# Ho·∫∑c ch·∫°y 1 l·ªánh (PowerShell)
@("kol.discovery.raw", "kol.profiles.raw", "kol.videos.raw", "kol.comments.raw", "kol.products.raw") | ForEach-Object { docker exec kol-redpanda rpk topic create $_ --partitions 4 }
```

### List Topics
```bash
docker exec kol-redpanda rpk topic list
```

### Describe Topic
```bash
docker exec kol-redpanda rpk topic describe kol.discovery.raw
docker exec kol-redpanda rpk topic describe kol.profiles.raw
docker exec kol-redpanda rpk topic describe kol.videos.raw
docker exec kol-redpanda rpk topic describe kol.comments.raw
docker exec kol-redpanda rpk topic describe kol.products.raw
```

### View Consumer Groups
```bash
# List all consumer groups
docker exec kol-redpanda rpk group list

# Describe specific group (xem offset, lag)
docker exec kol-redpanda rpk group describe kol-video-stats-v3
docker exec kol-redpanda rpk group describe kol-comment-extractor-v3
docker exec kol-redpanda rpk group describe kol-product-extractor-v3
```

### Reset Consumer Offset
```bash
# Reset v·ªÅ latest (ch·ªâ ƒë·ªçc message m·ªõi t·ª´ gi·ªù)
docker exec kol-redpanda rpk group seek kol-video-stats-v3 --to end
docker exec kol-redpanda rpk group seek kol-comment-extractor-v3 --to end
docker exec kol-redpanda rpk group seek kol-product-extractor-v3 --to end

# Reset v·ªÅ earliest (ƒë·ªçc l·∫°i t·ª´ ƒë·∫ßu)
docker exec kol-redpanda rpk group seek kol-video-stats-v3 --to start
```

### Produce Test Message
```bash
docker exec -it kol-redpanda rpk topic produce kol.discovery.raw
# Type message, press Ctrl+C to exit
```

### Consume Messages
```bash
docker exec -it kol-redpanda rpk topic consume kol.discovery.raw --offset newest
docker exec -it kol-redpanda rpk topic consume kol.profiles.raw --offset newest
```

---

## ‚ö° Spark Operations

### Submit Kafka ‚Üí MinIO ETL (Batch Mode)
```powershell
docker exec kol-spark-master /opt/spark/bin/spark-submit `
    --master spark://spark-master:7077 `
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,org.apache.hadoop:hadoop-aws:3.3.4 `
    --conf spark.hadoop.fs.s3a.endpoint=http://sme-minio:9000 `
    --conf spark.hadoop.fs.s3a.access.key=minio `
    --conf spark.hadoop.fs.s3a.secret.key=minio123 `
    --conf spark.hadoop.fs.s3a.path.style.access=true `
    /opt/spark/work-dir/streaming/spark_jobs/kafka_to_iceberg_simple.py --mode batch
```

### Submit Bronze ‚Üí Silver ETL
```bash
docker exec kol-spark-master spark-submit \
  --master spark://spark-master:7077 \
  /opt/spark/work-dir/batch/etl/bronze_to_silver.py
```

### Submit Product Tracker Batch Job
```bash
docker exec kol-spark-master spark-submit \
  --master spark://spark-master:7077 \
  /opt/spark/work-dir/batch/feature_store/product_tracker.py
```

### Check Spark UI
```
# Spark Master UI: http://localhost:8084
# Spark History: http://localhost:18080
```

---

## ü§ñ Model Training

### Train Trust Model
```bash
docker exec kol-trainer python -m models.trust.train_xgb
```

### Train Success Model
```bash
docker exec kol-trainer python -m models.success.train_lgbm
```

### Register Model in MLflow
```bash
docker exec kol-trainer python -m models.registry.model_versioning
```

---

## üß™ Testing

### Test API Endpoints
```bash
# Health check
curl http://localhost:8080/healthz

# Get KOL trust score
curl -X POST http://localhost:8080/api/v1/kol/score \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer dev-token-change-in-production" \
  -d '{
    "kol_id": "kol123",
    "features": {
      "follower_count": 50000,
      "engagement_rate": 0.05,
      "sentiment_score": 0.8
    }
  }'

# Get success forecast
curl -X POST http://localhost:8080/api/v1/forecast/success \
  -H "Content-Type: application/json" \
  -H "Authorization: Bearer dev-token-change-in-production" \
  -d '{
    "kol_id": "kol123",
    "campaign_id": "camp456",
    "horizon_days": 7
  }'
```

---

## üßπ Cleanup

### Remove All KOL Containers
```bash
make down-kol
```

### Remove Volumes (‚ö†Ô∏è Deletes Data)
```bash
docker compose -f infra/docker-compose.kol.yml down -v
```

### Remove All Stopped Containers
```bash
docker container prune
```

### Remove Unused Images
```bash
docker image prune -a
```

---

## üÜò Emergency Commands

### Force Restart Everything
```bash
make down-kol
docker system prune -f
make check-sme
make up-kol
```

### View All Logs (Last 100 Lines)
```bash
docker compose -f infra/docker-compose.kol.yml logs --tail=100
```

### Kill All KOL Containers
```bash
docker ps | grep kol- | awk '{print $1}' | xargs docker kill
```

---

## üìö Help

### Show All Available Commands
```bash
make help
```

### Docker Compose Help
```bash
docker compose --help
docker compose -f infra/docker-compose.kol.yml --help
```

---

## üîó Related Documents

- **docs/guides/PARALLEL_SCRAPING_GUIDE.md**: H∆∞·ªõng d·∫´n ch·∫°y parallel scrapers
- **MIGRATION_TO_SHARED_INFRA.md**: Complete migration guide
- **SHARED_INFRASTRUCTURE_GUIDE.md**: Connection architecture
- **QUICKSTART.md**: Step-by-step tutorial
- **Makefile**: All commands (`make help`)

---

## üìã Workflow T√≥m T·∫Øt

### üöÄ C√°ch 1: One-Click Start (Recommended)
```powershell
# Start to√†n b·ªô platform
.\scripts\start_full_platform.ps1

# Ch·∫°y r·ªìi ng·ªìi u·ªëng tr√† üçµ
# Platform s·∫Ω t·ª± ƒë·ªông:
# 1. Check infrastructure
# 2. Start Spark Streaming ‚Üí Redis
# 3. Start Discovery (m·ªói 2h t√¨m KOL m·ªõi)
# 4. Start Workers (delay 240s r·ªìi m·ªõi ch·∫°y)
# 5. Start Metrics Refresh (m·ªói 5min t√≠nh velocity)
```

### üîß C√°ch 2: Ch·∫°y Th·ªß C√¥ng t·ª´ng B∆∞·ªõc

**B∆∞·ªõc 1: Start Spark Streaming Job**
```powershell
# C√†i redis (1 l·∫ßn)
docker exec -u root kol-spark-master pip install redis

# Start streaming job
docker exec -d kol-spark-master /opt/spark/bin/spark-submit `
    --master spark://spark-master:7077 `
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0 `
    /opt/spark-jobs/kafka_trending_stream.py
```

**B∆∞·ªõc 2: Ch·∫°y Discovery + 5 Workers Song Song**
```powershell
# Terminal 1: Discovery (m·ªói 2 ti·∫øng)
py -m ingestion.sources.kol_scraper daemon --discovery-only --interval 7200

# Terminal 2: Metrics Refresh (m·ªói 5 ph√∫t)
py -m ingestion.sources.metrics_refresh --interval 300

# Terminal 3: Video Stats (delay 240s)
py -m ingestion.consumers.video_stats_worker --max-videos 20 --start-delay 240

# Terminal 4: Comments (optional)
py -m ingestion.consumers.comment_extractor --max-comments 50 --start-delay 240

# Terminal 5: Products
py -m ingestion.consumers.product_extractor --max-videos 20 --start-delay 240
```

**B∆∞·ªõc 3: Monitor**
```powershell
# Check Redis c√≥ data
docker exec kol-redis redis-cli KEYS "streaming_scores:*"

# Check Spark UI
http://localhost:8084
```

### üìä Optional: Ch·∫°y Spark ETL (load Kafka ‚Üí MinIO)
```powershell
docker exec kol-spark-master /opt/spark/bin/spark-submit `
    --master spark://spark-master:7077 `
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,org.apache.iceberg:iceberg-spark-runtime-3.5_2.12:1.4.2,org.apache.hadoop:hadoop-aws:3.3.4 `
    --conf spark.hadoop.fs.s3a.endpoint=http://sme-minio:9000 `
    --conf spark.hadoop.fs.s3a.access.key=minio `
    --conf spark.hadoop.fs.s3a.secret.key=minio123 `
    --conf spark.hadoop.fs.s3a.path.style.access=true `
    /opt/spark/work-dir/streaming/spark_jobs/kafka_to_iceberg_simple.py --mode batch
```

### üì¶ Optional: Ch·∫°y Product Tracker (track sold_count changes)
```powershell
# Schedule: 2-3 l·∫ßn/ng√†y
python -m batch.feature_store.product_tracker --local
```

---

**Tip**: Bookmark this file for quick reference during development! üöÄ
