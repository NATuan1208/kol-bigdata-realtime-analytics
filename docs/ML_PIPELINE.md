# ğŸ¤– KOL Trust Score - Machine Learning Pipeline

## IE212 - Big Data Analytics | UIT 2025

---

## ğŸ“‹ Má»¥c Lá»¥c

1. [Tá»•ng Quan BÃ i ToÃ¡n](#-tá»•ng-quan-bÃ i-toÃ¡n)
2. [Kiáº¿n TrÃºc ML Pipeline](#-kiáº¿n-trÃºc-ml-pipeline)
3. [Dataset & Features](#-dataset--features)
4. [Model Architecture](#-model-architecture)
5. [Training Results](#-training-results)
6. [Model Evaluation](#-model-evaluation)
7. [Feature Importance](#-feature-importance)
8. [HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng](#-hÆ°á»›ng-dáº«n-sá»­-dá»¥ng)
9. [Model Serving](#-model-serving)

---

## ğŸ¯ Tá»•ng Quan BÃ i ToÃ¡n

### Business Problem

Detect **KOL khÃ´ng Ä‘Ã¡ng tin (Untrustworthy KOLs)** - nhá»¯ng ngÆ°á»i cÃ³ hÃ nh vi:
- ğŸ¤– Sá»­ dá»¥ng **fake followers** (mua followers áº£o)
- ğŸ“ˆ **Suspicious growth patterns** (tÄƒng followers báº¥t thÆ°á»ng)
- ğŸ“‰ **Low engagement vá»›i high followers** (nhiá»u followers nhÆ°ng Ã­t tÆ°Æ¡ng tÃ¡c)
- âš™ï¸ **Bot-like activity** (hoáº¡t Ä‘á»™ng nhÆ° bot)

### ML Task Definition

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BINARY CLASSIFICATION TASK                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   Input:  29 engineered features tá»« KOL profile & activity              â”‚
â”‚                                                                         â”‚
â”‚   Output: Trust Score (0-100)                                           â”‚
â”‚           â”œâ”€â”€ 80-100: Highly Trustworthy âœ…                             â”‚
â”‚           â”œâ”€â”€ 60-79:  Moderately Trustworthy                            â”‚
â”‚           â”œâ”€â”€ 40-59:  Needs Review âš ï¸                                   â”‚
â”‚           â””â”€â”€ 0-39:   Likely Untrustworthy âŒ                           â”‚
â”‚                                                                         â”‚
â”‚   Label:  is_untrustworthy                                              â”‚
â”‚           â”œâ”€â”€ 0 = Trustworthy KOL (authentic, organic engagement)       â”‚
â”‚           â””â”€â”€ 1 = Untrustworthy KOL (fake followers, bot patterns)      â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Dataset Origin

**Source**: Twitter Human-Bot Detection Dataset (Kaggle)

**Semantic Re-mapping cho bÃ i toÃ¡n KOL Trust:**

| Original Label | Re-mapped Label | Interpretation |
|----------------|-----------------|----------------|
| `is_bot = 1` | `is_untrustworthy = 1` | KOL khÃ´ng Ä‘Ã¡ng tin |
| `is_bot = 0` | `is_untrustworthy = 0` | KOL Ä‘Ã¡ng tin |

**LÃ½ do features overlap (~80%):**

| Bot Patterns | Untrustworthy KOL Patterns |
|--------------|---------------------------|
| F/F ratio báº¥t thÆ°á»ng | Fake followers follow ratio |
| Account age ngáº¯n + followers tÄƒng nhanh | Mua followers â†’ tÄƒng Ä‘á»™t biáº¿n |
| Default profile, no bio | Focus mua followers hÆ¡n build profile |
| Low engagement rate | Fake followers khÃ´ng tÆ°Æ¡ng tÃ¡c |
| High posting frequency | DÃ¹ng bot Ä‘á»ƒ post |

---

## ğŸ—ï¸ Kiáº¿n TrÃºc ML Pipeline

### End-to-End Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           ML PIPELINE ARCHITECTURE                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚   MinIO     â”‚    â”‚   Feature   â”‚    â”‚   Model     â”‚    â”‚     Ensemble        â”‚  â”‚
â”‚  â”‚  (Parquet)  â”‚â”€â”€â”€â–¶â”‚ Engineering â”‚â”€â”€â”€â–¶â”‚  Training   â”‚â”€â”€â”€â–¶â”‚    Stacking         â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚        â”‚                   â”‚                  â”‚                      â”‚             â”‚
â”‚        â–¼                   â–¼                  â–¼                      â–¼             â”‚
â”‚   37,438 records     29 features      3 base models         Calibrated Score      â”‚
â”‚                                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚                         TRAINING INFRASTRUCTURE                              â”‚  â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚  â”‚
â”‚  â”‚  â”‚kol-trainer  â”‚  â”‚   MLflow    â”‚  â”‚   MinIO     â”‚  â”‚    kol-api          â”‚  â”‚  â”‚
â”‚  â”‚  â”‚ (Training)  â”‚  â”‚ (Registry)  â”‚  â”‚ (Artifacts) â”‚  â”‚   (Serving)         â”‚  â”‚  â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Docker Services

| Container | Role | Image | Port |
|-----------|------|-------|------|
| **kol-trainer** | ML Training | `infra-trainer:latest` | - |
| **kol-api** | Model Serving | `infra-api:latest` | 8000 |
| **mlflow** | Model Registry | `mlflow:latest` | 5000 |
| **sme-minio** | Data & Artifacts | `minio:latest` | 9000 |

### Pre-installed ML Libraries (kol-trainer)

```
âœ… XGBoost 3.1.1
âœ… LightGBM 4.6.0
âœ… scikit-learn 1.7.2
âœ… PyTorch 2.1+ (for PhoBERT)
âœ… Transformers (HuggingFace)
âœ… MLflow 2.9+
âœ… SHAP
âœ… Optuna
```

---

## ğŸ“Š Dataset & Features

### Dataset Statistics

| Metric | Value |
|--------|-------|
| **Total Records** | 37,438 |
| **Training Set** | 29,950 (80%) |
| **Test Set** | 7,488 (20%) |
| **Features** | 29 |
| **Target** | Binary (0/1) |

### Label Distribution

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   LABEL DISTRIBUTION                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Trustworthy (label=0)     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  66.8%     â”‚
â”‚  25,013 samples                                             â”‚
â”‚                                                             â”‚
â”‚  Untrustworthy (label=1)   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ           33.2%      â”‚
â”‚  12,425 samples                                             â”‚
â”‚                                                             â”‚
â”‚  Class Imbalance Ratio: 2.01:1                              â”‚
â”‚  scale_pos_weight (XGBoost): 2.013                          â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Feature Categories (29 Features)

#### 1ï¸âƒ£ Log Transforms (5 features)
| Feature | Description | Formula |
|---------|-------------|---------|
| `log_followers` | Log cá»§a followers | log(followers + 1) |
| `log_following` | Log cá»§a following | log(following + 1) |
| `log_posts` | Log cá»§a posts | log(posts + 1) |
| `log_favorites` | Log cá»§a favorites | log(favorites + 1) |
| `log_account_age` | Log cá»§a tuá»•i account | log(account_age + 1) |

#### 2ï¸âƒ£ Ratio Features (2 features)
| Feature | Description | Range |
|---------|-------------|-------|
| `followers_following_ratio_capped` | F/F ratio (capped) | [0, 10000] |
| `posts_per_day_capped` | Posts/day (capped) | [0, 50] |

#### 3ï¸âƒ£ Behavioral Scores (6 features)
| Feature | Description |
|---------|-------------|
| `engagement_rate` | favorites / (posts + 1) |
| `activity_score` | Composite activity metric |
| `profile_completeness` | (has_bio + has_url + has_image) / 3 |
| `followers_per_day` | Growth rate |
| `posts_per_follower` | Content density |
| `following_per_day` | Following behavior |

#### 4ï¸âƒ£ Untrustworthy Indicators (5 features) â­
| Feature | Description | Trigger |
|---------|-------------|---------|
| `high_activity_flag` | Bot-like posting | posts_per_day > 20 |
| `low_engagement_high_posts` | Fake followers pattern | High posts, low engagement |
| `default_profile_score` | No customization | Default settings |
| `suspicious_growth` | Unnatural growth | followers_per_day anomaly |
| `fake_follower_indicator` | Likely fake followers | High F, low engagement |

#### 5ï¸âƒ£ Categorical Tiers (3 features)
| Feature | Categories |
|---------|------------|
| `followers_tier` | Nano(0), Micro(1), Mid(2), Macro(3), Mega(4) |
| `account_age_tier` | <1y(0), 1-2y(1), 2-5y(2), 5+y(3) |
| `activity_tier` | Inactive(0), Low(1), Medium(2), High(3) |

#### 6ï¸âƒ£ Interaction Features (4 features)
| Feature | Formula |
|---------|---------|
| `verified_followers_interaction` | verified Ã— log_followers |
| `profile_engagement_interaction` | profile_completeness Ã— engagement_rate |
| `age_activity_interaction` | log_account_age Ã— activity_score |
| `bio_length_norm` | Normalized bio length |

#### 7ï¸âƒ£ Binary Features (4 features)
| Feature | Type |
|---------|------|
| `has_bio` | Boolean |
| `has_url` | Boolean |
| `has_profile_image` | Boolean |
| `verified` | Boolean |

---

## ğŸ§  Model Architecture

### Ensemble Stacking Design

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         ENSEMBLE STACKING ARCHITECTURE                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                                     â”‚
â”‚                              INPUT: 29 Features                                     â”‚
â”‚                                      â”‚                                              â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                            â”‚
â”‚                    â”‚                 â”‚                 â”‚                            â”‚
â”‚                    â–¼                 â–¼                 â–¼                            â”‚
â”‚           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”‚
â”‚           â”‚   XGBoost   â”‚   â”‚  LightGBM   â”‚   â”‚ Isolation   â”‚                       â”‚
â”‚           â”‚ Classifier  â”‚   â”‚ Classifier  â”‚   â”‚   Forest    â”‚                       â”‚
â”‚           â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                       â”‚
â”‚                  â”‚                 â”‚                 â”‚                              â”‚
â”‚             P(untrust)        P(untrust)       Anomaly Score                        â”‚
â”‚                  â”‚                 â”‚                 â”‚                              â”‚
â”‚                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚                               â”‚                                                     â”‚
â”‚                               â–¼                                                     â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚                    â”‚   META-LEARNER      â”‚                                          â”‚
â”‚                    â”‚ Logistic Regression â”‚                                          â”‚
â”‚                    â”‚                     â”‚                                          â”‚
â”‚                    â”‚ Weights:            â”‚                                          â”‚
â”‚                    â”‚ â€¢ XGB:    6.79 â­   â”‚                                          â”‚
â”‚                    â”‚ â€¢ LGBM:   1.18      â”‚                                          â”‚
â”‚                    â”‚ â€¢ IForest: -0.38    â”‚                                          â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚                               â”‚                                                     â”‚
â”‚                               â–¼                                                     â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚                    â”‚   CALIBRATION       â”‚                                          â”‚
â”‚                    â”‚ Isotonic Regression â”‚                                          â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚                               â”‚                                                     â”‚
â”‚                               â–¼                                                     â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                          â”‚
â”‚                    â”‚   TRUST SCORE       â”‚                                          â”‚
â”‚                    â”‚     (0 - 100)       â”‚                                          â”‚
â”‚                    â”‚                     â”‚                                          â”‚
â”‚                    â”‚ 100 = Trustworthy   â”‚                                          â”‚
â”‚                    â”‚   0 = Untrustworthy â”‚                                          â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                          â”‚
â”‚                                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Model Descriptions

#### 1ï¸âƒ£ XGBoost Classifier
```python
XGBClassifier(
    n_estimators=150,
    max_depth=6,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    min_child_weight=3,
    gamma=0.1,
    reg_alpha=0.1,
    reg_lambda=1.0,
    scale_pos_weight=2.013,  # Handle class imbalance
    early_stopping_rounds=20
)
```

**Äáº·c Ä‘iá»ƒm:**
- Gradient Boosting vá»›i regularization máº¡nh
- **Level-wise tree growth** (grow theo tá»«ng level)
- Xá»­ lÃ½ tá»‘t missing values
- **Primary contributor** trong ensemble (weight = 6.79)

#### 2ï¸âƒ£ LightGBM Classifier
```python
LGBMClassifier(
    n_estimators=150,
    num_leaves=31,
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    min_child_samples=20,
    reg_alpha=0.1,
    reg_lambda=0.1,
    class_weight='balanced'
)
```

**Äáº·c Ä‘iá»ƒm:**
- **Leaf-wise tree growth** (grow leaf cÃ³ loss reduction lá»›n nháº¥t)
- Histogram-based (nhanh hÆ¡n XGBoost)
- Xá»­ lÃ½ categorical features tá»‘t
- **Diverse patterns** so vá»›i XGBoost â†’ Ensemble diversity

#### 3ï¸âƒ£ Isolation Forest (Unsupervised)
```python
IsolationForest(
    n_estimators=200,
    contamination=0.33,
    max_features=1.0,
    bootstrap=False,
    random_state=42
)
```

**Äáº·c Ä‘iá»ƒm:**
- **Unsupervised anomaly detection**
- KhÃ´ng dÃ¹ng labels khi training
- Detect **novel patterns** mÃ  supervised models cÃ³ thá»ƒ miss
- Output: Anomaly score (0-1)
- **Negative contribution** trong ensemble (weight = -0.38)
  - Penalize cÃ¡c patterns anomaly mÃ  khÃ´ng match vá»›i labels

#### 4ï¸âƒ£ Meta-Learner: Logistic Regression
```python
LogisticRegression(
    C=1.0,
    max_iter=1000,
    random_state=42
)
```

**Role**: Combine predictions tá»« 3 base models

**Learned Weights:**
| Model | Weight | Interpretation |
|-------|--------|----------------|
| XGBoost | **6.79** | Main predictor |
| LightGBM | 1.18 | Supporting predictor |
| IsolationForest | -0.38 | Anomaly penalty |

---

## ğŸ“ˆ Training Results

### Model Performance Summary

| Model | Accuracy | Precision | Recall | F1-Score | ROC-AUC | PR-AUC |
|-------|----------|-----------|--------|----------|---------|--------|
| **XGBoost (Baseline)** | 87.42% | 79.57% | 83.54% | 0.8151 | 0.9403 | 0.9091 |
| **XGBoost + Optuna** | 87.62% | 85.97% | 76.26% | 0.8081 | **0.9418** | - |
| **LightGBM (Baseline)** | 87.66% | 79.89% | 83.94% | 0.8187 | 0.9406 | 0.9094 |
| **ğŸ† LightGBM + Optuna** | **88.39%** | **86.10%** | 77.55% | 0.8160 | **0.9423** | - |
| **IsolationForest** | 43.96% | 15.18% | 15.01% | 0.1510 | 0.4012 | - |
| **Ensemble (Baseline)** | 88.21% | 83.29% | 80.64% | 0.8195 | 0.9403 | 0.9069 |

> **ğŸ¯ Best Model: LightGBM + Optuna** vá»›i ROC-AUC = 0.9423, Accuracy = 88.39%

### Performance Visualization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ROC-AUC COMPARISON                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                        â”‚
â”‚  LGBM+Optuna  â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ 0.9423 ğŸ† â”‚
â”‚  XGB+Optuna   â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  0.9418    â”‚
â”‚  LightGBM     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  0.9406    â”‚
â”‚  XGBoost      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  0.9403    â”‚
â”‚  Ensemble     â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  0.9403    â”‚
â”‚  IForest      â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                              0.4012    â”‚
â”‚                                                                        â”‚
â”‚               0.0    0.2    0.4    0.6    0.8    1.0                   â”‚
â”‚                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Confusion Matrix (Ensemble)

```
                      Predicted
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ Trust=0 â”‚ Trust=1 â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚ Actual=0  â”‚  4,601  â”‚   402   â”‚  â†’ True Negatives / False Positives
True â”‚           â”‚  (TN)   â”‚  (FP)   â”‚
     â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
     â”‚ Actual=1  â”‚   481   â”‚  2,004  â”‚  â†’ False Negatives / True Positives
     â”‚           â”‚  (FN)   â”‚  (TP)   â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Metrics:
â”œâ”€â”€ Precision = TP/(TP+FP) = 2,004/(2,004+402) = 83.29%
â”œâ”€â”€ Recall    = TP/(TP+FN) = 2,004/(2,004+481) = 80.64%
â”œâ”€â”€ Accuracy  = (TN+TP)/Total = (4,601+2,004)/7,488 = 88.21%
â””â”€â”€ F1-Score  = 2Ã—PÃ—R/(P+R) = 81.95%
```

### Trust Score Distribution

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    TRUST SCORE DISTRIBUTION                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                        â”‚
â”‚  Trustworthy KOLs (n=5,003):                                           â”‚
â”‚  â”œâ”€â”€ Mean Score: 88.8 / 100                                            â”‚
â”‚  â”œâ”€â”€ Std Dev: 21.1                                                     â”‚
â”‚  â””â”€â”€ Distribution: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘  (skew right)        â”‚
â”‚                                                                        â”‚
â”‚  Untrustworthy KOLs (n=2,485):                                         â”‚
â”‚  â”œâ”€â”€ Mean Score: 22.2 / 100                                            â”‚
â”‚  â”œâ”€â”€ Std Dev: 31.6                                                     â”‚
â”‚  â””â”€â”€ Distribution: â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘â–‘  (skew left)         â”‚
â”‚                                                                        â”‚
â”‚  Score Interpretation:                                                 â”‚
â”‚  â”œâ”€â”€ 80-100: Highly Trustworthy âœ… (recommend for campaigns)           â”‚
â”‚  â”œâ”€â”€ 60-79:  Moderately Trustworthy (proceed with caution)             â”‚
â”‚  â”œâ”€â”€ 40-59:  Needs Review âš ï¸ (manual verification required)            â”‚
â”‚  â””â”€â”€ 0-39:   Likely Untrustworthy âŒ (avoid for campaigns)             â”‚
â”‚                                                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”¬ Optuna Hyperparameter Tuning

### Táº¡i sao dÃ¹ng Optuna?

**Optuna** lÃ  framework Bayesian Optimization hiá»‡n Ä‘áº¡i, Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i:
- âœ… **5000+ citations** trong research papers
- âœ… DÃ¹ng bá»Ÿi **Toyota, Sony, Preferred Networks**
- âœ… Top choice cá»§a **Kaggle Grandmasters**

### So sÃ¡nh vá»›i Manual Tuning

| Approach | CÃ¡ch hoáº¡t Ä‘á»™ng | Hiá»‡u quáº£ |
|----------|---------------|----------|
| **Manual** | ÄoÃ¡n params, thá»­ sai | âŒ Chá»§ quan, khÃ´ng tá»‘i Æ°u |
| **Grid Search** | Thá»­ Táº¤T Cáº¢ combinations | âš ï¸ Tá»‘n thá»i gian (O(n^k)) |
| **Random Search** | Random sampling | âš ï¸ May rá»§i |
| **Optuna (Bayesian)** | Há»c tá»« trials trÆ°á»›c | âœ… ThÃ´ng minh, nhanh 10x |

### Optuna Configuration

```python
# Configuration used:
N_TRIALS = 50           # Sá»‘ trials optimization
N_CV_FOLDS = 5          # Stratified K-Fold Cross-Validation
EARLY_STOPPING = 30     # Rounds for early stopping
NUM_BOOST_ROUND = 500   # Max boosting iterations

# Hyperparameter Search Space:
search_space = {
    'max_depth': [3, 10],           # Tree depth
    'learning_rate': [0.01, 0.2],   # Learning rate (log scale)
    'num_leaves': [20, 150],        # LightGBM leaf nodes
    'min_child_samples': [10, 100], # Min samples per leaf
    'subsample': [0.6, 1.0],        # Row sampling
    'colsample_bytree': [0.6, 1.0], # Column sampling
    'reg_alpha': [1e-6, 5.0],       # L1 regularization
    'reg_lambda': [1e-6, 5.0],      # L2 regularization
}
```

### LightGBM Optuna Results

```
======================================================================
ğŸ“ˆ OPTUNA OPTIMIZATION RESULTS
======================================================================

ğŸ† Best Trial: #35/50
   Best CV ROC-AUC: 0.9414

ğŸ“‹ Best Hyperparameters Found:
   colsample_bytree:   0.846971
   learning_rate:      0.019015
   max_depth:          7
   min_child_samples:  28
   min_gain_to_split:  0.262514
   num_leaves:         124
   reg_alpha:          1.445124
   reg_lambda:         0.002523
   subsample:          0.797141
   subsample_freq:     2

ğŸ“Š Final Test Performance:
   ROC-AUC:   0.9423 (+0.17% vs baseline)
   F1-Score:  0.8160
   Accuracy:  88.39% (+0.73% vs baseline)
   Precision: 86.10%
   Recall:    77.55%

â±ï¸ Training Time: 5.6 minutes (50 trials Ã— 5-fold CV)
```

### XGBoost Optuna Results

```
======================================================================
ğŸ“ˆ OPTUNA OPTIMIZATION RESULTS
======================================================================

ğŸ† Best Trial: #46/50
   Best CV ROC-AUC: 0.9413

ğŸ“‹ Best Hyperparameters Found:
   colsample_bytree:  0.743243
   gamma:             0.072098
   learning_rate:     0.024657
   max_depth:         5
   min_child_weight:  8
   reg_alpha:         4.091159
   reg_lambda:        0.006699
   subsample:         0.704304

ğŸ“Š Final Test Performance:
   ROC-AUC:   0.9418 (+0.15% vs baseline)
   F1-Score:  0.8081
   Accuracy:  87.62%
   Precision: 85.97%
   Recall:    76.26%

â±ï¸ Training Time: 10.9 minutes (50 trials Ã— 5-fold CV)
```

### Optuna Trials Visualization

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    OPTUNA OPTIMIZATION HISTORY                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  AUC                                                                    â”‚
â”‚  0.945â”‚                                                                 â”‚
â”‚       â”‚                                    â”Œâ”€â”€â”€â”€â”€Best: 0.9414           â”‚
â”‚  0.942â”‚       â—    â—  â—â—    â—  â—â— â—  â—â—   â—â— â—â—                         â”‚
â”‚       â”‚     â—  â—â—â—â— â—   â—â—â—â— â—â—    â—â—  â—â—â—    â—â—â—                       â”‚
â”‚  0.939â”‚    â—â—                                                           â”‚
â”‚       â”‚   â—                                                             â”‚
â”‚  0.936â”‚  â—                                                              â”‚
â”‚       â”‚ â—                                                               â”‚
â”‚  0.933â”‚â—                                                                â”‚
â”‚       â”‚                                                                 â”‚
â”‚  0.920â”‚  â—  (Trial 2: Bad params)                                       â”‚
â”‚       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€    â”‚
â”‚        0    5    10   15   20   25   30   35   40   45   50  Trial      â”‚
â”‚                                                                         â”‚
â”‚  Observation: Optuna nhanh chÃ³ng tÃ¬m Ä‘Æ°á»£c vÃ¹ng params tá»‘t sau ~10       â”‚
â”‚  trials, sau Ä‘Ã³ tinh chá»‰nh Ä‘á»ƒ Ä‘áº¡t optimum táº¡i trial 35.                 â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Improvement Analysis

| Metric | Baseline | + Optuna | Improvement |
|--------|----------|----------|-------------|
| **LightGBM ROC-AUC** | 0.9406 | **0.9423** | +0.17% |
| **LightGBM Accuracy** | 87.66% | **88.39%** | +0.73% |
| **XGBoost ROC-AUC** | 0.9403 | **0.9418** | +0.15% |
| **XGBoost Accuracy** | 87.42% | **87.62%** | +0.20% |

> **Káº¿t luáº­n:** Optuna cáº£i thiá»‡n performance má»™t cÃ¡ch nháº¥t quÃ¡n. DÃ¹ improvement nhá» (~0.2%), nhÆ°ng vá»›i bÃ i toÃ¡n classification nÃ y, má»—i 0.1% Ä‘á»u cÃ³ Ã½ nghÄ©a cho viá»‡c detect untrustworthy KOLs.

---

## ğŸ¯ Feature Importance

### XGBoost Feature Importance (Top 15)

| Rank | Feature | Importance | Visual |
|------|---------|------------|--------|
| 1 | `verified` | 0.1895 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ |
| 2 | `log_followers` | 0.1565 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ |
| 3 | `followers_tier` | 0.0820 | â–ˆâ–ˆâ–ˆâ–ˆ |
| 4 | `activity_tier` | 0.0786 | â–ˆâ–ˆâ–ˆ |
| 5 | `following_per_day` | 0.0482 | â–ˆâ–ˆ |
| 6 | `engagement_rate` | 0.0395 | â–ˆ |
| 7 | `followers_per_day` | 0.0375 | â–ˆ |
| 8 | `verified_followers_interaction` | 0.0348 | â–ˆ |
| 9 | `log_favorites` | 0.0337 | â–ˆ |
| 10 | `profile_engagement_interaction` | 0.0309 | â–ˆ |
| 11 | `age_activity_interaction` | 0.0276 | â–ˆ |
| 12 | `posts_per_day_capped` | 0.0253 | â–ˆ |
| 13 | `log_account_age` | 0.0252 | â–ˆ |
| 14 | `followers_following_ratio_capped` | 0.0227 | â–ˆ |
| 15 | `log_following` | 0.0215 | â–ˆ |

### LightGBM Feature Importance - Gain (Top 15)

| Rank | Feature | Gain | Visual |
|------|---------|------|--------|
| 1 | `log_followers` | 31,439 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ |
| 2 | `followers_per_day` | 14,549 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ |
| 3 | `engagement_rate` | 10,647 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ |
| 4 | `log_account_age` | 10,221 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ |
| 5 | `log_favorites` | 8,774 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ |
| 6 | `log_following` | 7,273 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ |
| 7 | `profile_engagement_interaction` | 6,707 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ |
| 8 | `posts_per_follower` | 5,297 | â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ |
| 9 | `verified_followers_interaction` | 4,591 | â–ˆâ–ˆâ–ˆâ–ˆ |
| 10 | `following_per_day` | 4,374 | â–ˆâ–ˆâ–ˆâ–ˆ |

### Key Insights tá»« Feature Importance

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    KEY INSIGHTS                                         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  ğŸ¯ TOP PREDICTORS:                                                     â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                      â”‚
â”‚  1. verified - Verification status lÃ  strong indicator                  â”‚
â”‚     â†’ Unverified accounts more likely to be untrustworthy               â”‚
â”‚                                                                         â”‚
â”‚  2. log_followers - Follower count (log-transformed)                    â”‚
â”‚     â†’ Extreme values indicate suspicious patterns                       â”‚
â”‚                                                                         â”‚
â”‚  3. followers_per_day - Growth rate                                     â”‚
â”‚     â†’ Rapid growth suggests purchased followers                         â”‚
â”‚                                                                         â”‚
â”‚  4. engagement_rate - Engagement per post                               â”‚
â”‚     â†’ Low engagement with high followers = fake followers               â”‚
â”‚                                                                         â”‚
â”‚  ğŸ“Š MODEL AGREEMENT:                                                    â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                                   â”‚
â”‚  â€¢ Both XGBoost vÃ  LightGBM agree on top features                       â”‚
â”‚  â€¢ log_followers, engagement_rate, followers_per_day                    â”‚
â”‚  â€¢ verified status crucial for prediction                               â”‚
â”‚                                                                         â”‚
â”‚  ğŸ” DIFFERENT PERSPECTIVES:                                             â”‚
â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                                            â”‚
â”‚  â€¢ XGBoost: Focus on verified, followers_tier (categorical)             â”‚
â”‚  â€¢ LightGBM: Focus on continuous metrics (growth rates)                 â”‚
â”‚  â€¢ Ensemble captures BOTH perspectives                                  â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“– HÆ°á»›ng Dáº«n Sá»­ Dá»¥ng

### 1. Training Pipeline

```bash
# Step 1: VÃ o container trainer
docker exec -it kol-trainer bash

# Step 2a: Train baseline models
python -m models.trust.train_xgb      # Train XGBoost
python -m models.trust.train_lgbm     # Train LightGBM  
python -m models.trust.score_iforest  # Train Isolation Forest
python -m models.trust.stack_calibrate # Build Ensemble

# Step 2b: Train vá»›i Optuna tuning (Recommended â­)
python -m models.trust.train_xgb_optuna   # XGBoost + Optuna (~11 min)
python -m models.trust.train_lgbm_optuna  # LightGBM + Optuna (~6 min)

# Step 3: Evaluate táº¥t cáº£ models
python -m models.trust.evaluate --save-report

# Step 4: View reports
cat /app/models/reports/model_comparison.csv
cat /app/models/reports/lgbm_optuna_metrics.json
cat /app/models/reports/xgb_optuna_metrics.json
```

### 2. Quick Commands (tá»« host)

```bash
# Train all models (one-liner)
docker exec kol-trainer python -m models.trust.train_xgb && \
docker exec kol-trainer python -m models.trust.train_lgbm && \
docker exec kol-trainer python -m models.trust.score_iforest && \
docker exec kol-trainer python -m models.trust.stack_calibrate

# Evaluate with reports
docker exec kol-trainer python -m models.trust.evaluate --save-report
```

### 3. Using Makefile

```bash
# Train XGBoost
make train-xgb

# Train LightGBM  
make train-lgbm

# Full pipeline
make train-trust-models
```

### 4. Python API Usage

```python
# Load trained model
import joblib
from models.trust.data_loader import load_training_data, FEATURE_COLUMNS

# Load data
X_train, X_test, y_train, y_test = load_training_data()

# Load XGBoost model
xgb_model = joblib.load('models/artifacts/trust/xgb_trust_classifier_latest.joblib')

# Predict
y_pred = xgb_model.predict(X_test)
y_proba = xgb_model.predict_proba(X_test)[:, 1]

# Trust Score (using ensemble)
from models.trust.stack_calibrate import compute_trust_score
trust_scores = compute_trust_score(X_test)  # 0-100 scale
```

---

## ğŸŒ Model Serving

### API Endpoints (kol-api container)

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/healthz` | GET | Health check |
| `/predict/trust` | POST | Predict Trust Score |
| `/predict/batch` | POST | Batch prediction |
| `/models/info` | GET | Model metadata |

### Sample API Request

```bash
curl -X POST http://localhost:8000/predict/trust \
  -H "Content-Type: application/json" \
  -d '{
    "kol_id": "user_123",
    "followers_count": 50000,
    "following_count": 1000,
    "post_count": 500,
    "favorites_count": 25000,
    "account_age_days": 730,
    "verified": false,
    "has_bio": true,
    "has_url": true,
    "has_profile_image": true
  }'
```

### Sample Response

```json
{
  "kol_id": "user_123",
  "trust_score": 72.5,
  "is_untrustworthy": false,
  "confidence": 0.89,
  "risk_level": "moderate",
  "top_risk_factors": [
    "high_followers_following_ratio",
    "suspicious_growth_pattern"
  ],
  "recommendation": "Proceed with caution. Manual review recommended."
}
```

---

## ğŸ“ File Structure

```
models/
â”œâ”€â”€ trust/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_loader.py           # Load data tá»« MinIO
â”‚   â”œâ”€â”€ train_xgb.py             # XGBoost baseline training
â”‚   â”œâ”€â”€ train_xgb_optuna.py      # XGBoost + Optuna tuning â­
â”‚   â”œâ”€â”€ train_lgbm.py            # LightGBM baseline training
â”‚   â”œâ”€â”€ train_lgbm_optuna.py     # LightGBM + Optuna tuning â­
â”‚   â”œâ”€â”€ score_iforest.py         # Isolation Forest
â”‚   â”œâ”€â”€ stack_calibrate.py       # Ensemble stacking
â”‚   â”œâ”€â”€ evaluate.py              # Model evaluation
â”‚   â””â”€â”€ run_optuna_pipeline.py   # Run full Optuna pipeline
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ trust/
â”‚   â”‚   â”œâ”€â”€ xgb_trust_classifier_latest.joblib
â”‚   â”‚   â”œâ”€â”€ xgb_optuna_model.pkl          # Optuna-tuned XGBoost â­
â”‚   â”‚   â”œâ”€â”€ lgbm_trust_classifier_latest.joblib
â”‚   â”‚   â”œâ”€â”€ lgbm_optuna_model.pkl         # Optuna-tuned LightGBM â­
â”‚   â”‚   â”œâ”€â”€ lgbm_optuna_model.txt         # Native LightGBM format
â”‚   â”‚   â”œâ”€â”€ iforest_trust_anomaly_latest.joblib
â”‚   â”‚   â”œâ”€â”€ ensemble_trust_score_latest_meta.joblib
â”‚   â”‚   â””â”€â”€ *_metadata.json
â”‚   â””â”€â”€ optuna/
â”‚       â”œâ”€â”€ xgb_best_params.json          # Best XGBoost params
â”‚       â”œâ”€â”€ xgb_optuna_study.pkl          # Optuna study object
â”‚       â”œâ”€â”€ xgb_trials_history.csv        # All trials history
â”‚       â”œâ”€â”€ lgbm_best_params.json         # Best LightGBM params
â”‚       â”œâ”€â”€ lgbm_optuna_study.pkl         # Optuna study object
â”‚       â””â”€â”€ lgbm_trials_history.csv       # All trials history
â””â”€â”€ reports/
    â”œâ”€â”€ model_comparison.csv
    â”œâ”€â”€ full_metrics.json
    â”œâ”€â”€ xgb_optuna_metrics.json           # XGBoost Optuna results
    â”œâ”€â”€ xgb_optuna_feature_importance.csv
    â”œâ”€â”€ lgbm_optuna_metrics.json          # LightGBM Optuna results
    â””â”€â”€ lgbm_optuna_feature_importance.csv
```

---

## ğŸ“Š Model Artifacts

### Saved Files

| File | Size | Description |
|------|------|-------------|
| `xgb_trust_classifier_*.joblib` | ~2MB | XGBoost model |
| `lgbm_trust_classifier_*.joblib` | ~1MB | LightGBM model |
| `lgbm_trust_classifier_*.lgb` | ~500KB | Native LightGBM format |
| `iforest_trust_anomaly_*.joblib` | ~5MB | Isolation Forest |
| `iforest_trust_anomaly_*_scaler.joblib` | ~10KB | StandardScaler |
| `ensemble_trust_score_*_meta.joblib` | ~50KB | Meta-learner |
| `*_metadata.json` | ~5KB | Training metadata |

### Metadata Example

```json
{
  "model_name": "xgb_trust_classifier",
  "version": "20251127_021048",
  "training_date": "2025-11-27T02:10:48",
  "metrics": {
    "accuracy": 0.8742,
    "precision": 0.7957,
    "recall": 0.8354,
    "f1_score": 0.8151,
    "roc_auc": 0.9403
  },
  "feature_count": 29,
  "training_samples": 29950,
  "test_samples": 7488,
  "hyperparameters": {
    "n_estimators": 150,
    "max_depth": 6,
    "learning_rate": 0.1
  }
}
```

---

## ğŸ”® Future Improvements

### Phase 3: Model Enhancements

- [x] **Hyperparameter Tuning vá»›i Optuna** âœ… COMPLETED
  - âœ… 50 trials Ã— 5-fold Stratified CV
  - âœ… TPE (Tree-structured Parzen Estimator) sampler
  - âœ… XGBoost: ROC-AUC 0.9403 â†’ 0.9418 (+0.15%)
  - âœ… LightGBM: ROC-AUC 0.9406 â†’ 0.9423 (+0.17%)

- [ ] **SHAP Analysis**
  - Feature importance visualization
  - Individual prediction explanations

- [ ] **PhoBERT Integration**
  - NLP analysis cá»§a bio/content
  - Text-based untrustworthy detection

### Phase 4: MLOps

- [ ] **MLflow Integration**
  - Model versioning
  - Experiment tracking
  - Model registry

- [ ] **Model Monitoring**
  - Drift detection
  - Performance tracking
  - Automated retraining

- [ ] **A/B Testing**
  - Compare model versions
  - Gradual rollout

---

## ğŸ“š References

### Papers & Resources

1. **XGBoost**: Chen & Guestrin (2016). "XGBoost: A Scalable Tree Boosting System"
2. **LightGBM**: Ke et al. (2017). "LightGBM: A Highly Efficient Gradient Boosting Decision Tree"
3. **Isolation Forest**: Liu et al. (2008). "Isolation Forest"
4. **Bot Detection**: Cresci et al. (2020). "A Decade of Social Bot Detection"

### Dataset

- Twitter Human-Bot Detection Dataset (Kaggle)
- Re-mapped for KOL Trust Score prediction

---

*Last Updated: November 27, 2025*
*Author: KOL Analytics Team - IE212 UIT*
*Models trained on: kol-trainer container*
