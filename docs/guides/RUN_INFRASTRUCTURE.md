# üöÄ H∆∞·ªõng D·∫´n Ch·∫°y H·∫° T·∫ßng KOL Platform

## üìã T·ªïng Quan

H·ªá th·ªëng KOL Platform s·ª≠ d·ª•ng **Spark Structured Streaming** cho c·∫£ streaming v√† batch processing, v·ªõi architecture g·ªìm 2 layers:

### Base Platform Layer (SME Pulse)
- MinIO, PostgreSQL, Trino, Airflow, Hive Metastore, dbt

### KOL Extension Layer
- Redpanda (Kafka), Spark (Streaming + Batch), MLflow, Cassandra, Redis, API

---

## ‚ö° Quick Start (5 ph√∫t)

### B∆∞·ªõc 1: Kh·ªüi t·∫°o m√¥i tr∆∞·ªùng

```powershell
# Di chuy·ªÉn v√†o th∆∞ m·ª•c project
cd "d:\SinhVien\UIT_HocChinhKhoa\HK1 2025 - 2026\Bigdata_IE212\DoAn\kol-platform"

# T·∫°o network v√† environment files
make init
make network-create
```

### B∆∞·ªõc 2: Kh·ªüi ƒë·ªông to√†n b·ªô h·ªá th·ªëng

```powershell
# Kh·ªüi ƒë·ªông t·∫•t c·∫£ services (Base + KOL)
make up-kol
```

**ƒê·ª£i 3-5 ph√∫t** ƒë·ªÉ t·∫•t c·∫£ services kh·ªüi ƒë·ªông ho√†n t·∫•t.

### B∆∞·ªõc 3: Ki·ªÉm tra tr·∫°ng th√°i

```powershell
# Ki·ªÉm tra health c·ªßa services
make health

# Xem tr·∫°ng th√°i containers
make ps-all

# Xem logs
make logs-kol
```

### B∆∞·ªõc 4: Kh·ªüi t·∫°o d·ªØ li·ªáu

```powershell
# T·∫°o MinIO buckets
make init-buckets

# T·∫°o Kafka topics
make init-topics

# T·∫°o Cassandra keyspace v√† tables
docker exec -i kol-cassandra cqlsh < infra/scripts/init-cassandra.cql
```

---

## üåê Truy C·∫≠p Services

Sau khi kh·ªüi ƒë·ªông th√†nh c√¥ng, truy c·∫≠p c√°c services:

| Service | URL | Th√¥ng tin ƒëƒÉng nh·∫≠p |
|---------|-----|---------------------|
| **MinIO Console** | http://localhost:9001 | minioadmin / minioadmin123 |
| **Trino UI** | http://localhost:8080 | User: trino (no password) |
| **Airflow** | http://localhost:8081 | admin / admin123 |
| **Redpanda Console** | http://localhost:8082 | Kh√¥ng c·∫ßn ƒëƒÉng nh·∫≠p |
| **Spark Master UI** | http://localhost:8084 | Kh√¥ng c·∫ßn ƒëƒÉng nh·∫≠p |
| **Spark History** | http://localhost:18080 | Kh√¥ng c·∫ßn ƒëƒÉng nh·∫≠p |
| **MLflow UI** | http://localhost:5000 | Kh√¥ng c·∫ßn ƒëƒÉng nh·∫≠p |
| **API Swagger** | http://localhost:8080/docs | Kh√¥ng c·∫ßn ƒëƒÉng nh·∫≠p (dev) |
| **Jupyter Lab** | http://localhost:8888 | Kh√¥ng c·∫ßn ƒëƒÉng nh·∫≠p (dev) |

---

## üîß C√°c L·ªánh Th∆∞·ªùng D√πng

### Qu·∫£n l√Ω Infrastructure

```powershell
# Kh·ªüi ƒë·ªông
make up-kol          # Kh·ªüi ƒë·ªông t·∫•t c·∫£ (Base + KOL)
make up-base         # Ch·ªâ kh·ªüi ƒë·ªông Base platform
make up-kol-only     # Ch·ªâ kh·ªüi ƒë·ªông KOL stack (gi·∫£ s·ª≠ Base ƒë√£ ch·∫°y)

# D·ª´ng
make down-kol        # D·ª´ng KOL stack
make down-base       # D·ª´ng Base platform
make down-all        # D·ª´ng t·∫•t c·∫£

# Kh·ªüi ƒë·ªông l·∫°i
make restart-kol     # Kh·ªüi ƒë·ªông l·∫°i KOL stack
```

### Xem Logs

```powershell
# T·∫•t c·∫£ KOL services
make logs-kol

# Service c·ª• th·ªÉ
make logs-api
make logs-trainer
make logs-spark

# Base platform
make logs-base
```

### Truy c·∫≠p Containers

```powershell
# API container
make exec-api

# Trainer container
make exec-trainer

# PostgreSQL shell
make exec-postgres

# Redis CLI
make exec-redis

# Cassandra CQL shell
make exec-cassandra
```

### Ch·∫°y Training Jobs

```powershell
# Ch·∫°y training t·∫•t c·∫£ models
make train

# Ch·∫°y training model c·ª• th·ªÉ
make train-trust
make train-success

# Ho·∫∑c exec v√†o container v√† ch·∫°y tr·ª±c ti·∫øp
docker exec -it kol-trainer python -m models.trust.train_xgb
```

---

## üöÄ Ch·∫°y Spark Structured Streaming Jobs

### C√°ch 1: Submit job t·ª´ container

```powershell
# Exec v√†o Spark streaming container
docker exec -it kol-spark-streaming bash

# Submit Spark Structured Streaming job
spark-submit \
    --master spark://spark-master:7077 \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 \
    /opt/spark-jobs/features_stream.py
```

### C√°ch 2: Submit t·ª´ host machine

```powershell
# Submit job v√†o Spark cluster
docker exec kol-spark-streaming spark-submit \
    --master spark://spark-master:7077 \
    --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0,com.datastax.spark:spark-cassandra-connector_2.12:3.4.1 \
    /opt/spark-jobs/features_stream.py
```

### Monitor Streaming Job

- **Spark UI**: http://localhost:8084 (xem Streaming tab)
- **Logs**: `docker logs -f kol-spark-streaming`
- **Console output**: Metrics ƒë∆∞·ª£c in ra console khi ch·∫°y

---

## üîç Kh√°m Ph√° D·ªØ Li·ªáu

### Query v·ªõi Trino

```powershell
# M·ªü Trino CLI
docker exec -it base-trino trino

# List catalogs
SHOW CATALOGS;

# List schemas trong Iceberg
SHOW SCHEMAS FROM iceberg;

# T·∫°o test table
CREATE TABLE iceberg.silver.test_events (
    kol_id VARCHAR,
    event_type VARCHAR,
    event_time TIMESTAMP,
    impressions BIGINT
);
```

### Ki·ªÉm tra Kafka Topics

```powershell
# List topics
docker exec kol-redpanda rpk topic list

# Produce test message
docker exec -it kol-redpanda rpk topic produce events.social.raw
# Nh·∫≠p JSON message, v√≠ d·ª•:
# {"kol_id":"kol_001","event_type":"impression","event_time":"2025-11-13T10:00:00Z","impressions":100}

# Consume messages
docker exec kol-redpanda rpk topic consume events.social.raw --num 10
```

### Query Cassandra

```powershell
# M·ªü CQL shell
docker exec -it kol-cassandra cqlsh

# Use keyspace
USE kol_metrics;

# Describe tables
DESCRIBE TABLES;

# Query realtime metrics
SELECT * FROM kol_realtime_metrics LIMIT 10;

# Query v·ªõi ƒëi·ªÅu ki·ªán
SELECT * FROM kol_realtime_metrics WHERE kol_id = 'kol_001' LIMIT 10;
```

### Ki·ªÉm tra Redis Cache

```powershell
# M·ªü Redis CLI
docker exec -it kol-redis redis-cli

# List all keys
KEYS *

# Get value
GET some_key

# Monitor real-time commands
MONITOR
```

---

## üß™ Test API

### Health Check

```powershell
# PowerShell
Invoke-WebRequest http://localhost:8080/healthz

# ho·∫∑c d√πng curl (n·∫øu c√≥ WSL)
curl http://localhost:8080/healthz
```

### API Documentation

M·ªü browser v√† v√†o: **http://localhost:8080/docs**

ƒê√¢y l√† Swagger UI interactive, c√≥ th·ªÉ test t·∫•t c·∫£ endpoints.

---

## üêõ Troubleshooting

### Services kh√¥ng kh·ªüi ƒë·ªông?

```powershell
# Ki·ªÉm tra logs
make logs-kol

# Ki·ªÉm tra Docker resources
docker info

# ƒê·∫£m b·∫£o c√≥ √≠t nh·∫•t 8GB RAM allocated cho Docker

# Kh·ªüi ƒë·ªông l·∫°i Docker Desktop
```

### Port conflicts?

```powershell
# Ki·ªÉm tra port ƒëang d√πng (v√≠ d·ª• 8080)
netstat -ano | findstr :8080

# S·ª≠a port trong .env.kol
notepad .env.kol
# Thay ƒë·ªïi API_PORT=8090

# Kh·ªüi ƒë·ªông l·∫°i
make restart-kol
```

### Network issues?

```powershell
# Ki·ªÉm tra network
docker network ls | findstr data-platform-net

# Recreate network
make network-remove
make network-create

# Kh·ªüi ƒë·ªông l·∫°i services
make restart-kol
```

### Out of memory?

1. TƒÉng Docker Desktop memory l√™n 12GB+
2. Gi·∫£m s·ªë worker replicas trong `docker-compose.kol.yml`
3. Gi·∫£m Spark worker memory trong `.env.kol`

---

## üìä Monitoring

### Ki·ªÉm tra tr·∫°ng th√°i services

```powershell
# Health check t·∫•t c·∫£
make health

# Xem resource usage
make stats
# ho·∫∑c
docker stats --no-stream
```

### Xem Spark UI

- **Master UI**: http://localhost:8084
  - Workers status
  - Running applications
  - Completed applications
  
- **Streaming Query Progress**: Check trong Spark UI > Streaming tab

### Xem MLflow Experiments

- **MLflow UI**: http://localhost:5000
  - Experiments list
  - Model registry
  - Artifacts

---

## üõë D·ª´ng v√† Cleanup

### D·ª´ng services

```powershell
# D·ª´ng KOL stack (gi·ªØ Base platform)
make down-kol

# D·ª´ng t·∫•t c·∫£
make down-all
```

### Cleanup (x√≥a volumes)

```powershell
# Cleanup containers v√† volumes
make clean

# Deep clean (bao g·ªìm images)
make clean-all
```

**‚ö†Ô∏è C·∫£nh b√°o**: `make clean` s·∫Ω x√≥a t·∫•t c·∫£ d·ªØ li·ªáu trong volumes!

---

## üìö T√†i Li·ªáu Chi Ti·∫øt

| Document | M√¥ t·∫£ |
|----------|-------|
| **[QUICKSTART.md](QUICKSTART.md)** | Quick start guide chi ti·∫øt |
| **[INFRASTRUCTURE.md](INFRASTRUCTURE.md)** | T√†i li·ªáu infrastructure ƒë·∫ßy ƒë·ªß |
| **[PROJECT_ROADMAP.md](PROJECT_ROADMAP.md)** | L·ªô tr√¨nh tri·ªÉn khai project |
| **[README.md](README.md)** | T·ªïng quan project |

---

## üéØ Next Steps

Sau khi ch·∫°y ƒë∆∞·ª£c infrastructure:

1. **Implement Ingestion**: T·∫°o connectors ƒë·ªÉ ingest data t·ª´ social platforms
2. **Develop Streaming Jobs**: Implement Spark Structured Streaming jobs
3. **Train Models**: Train Trust & Success models
4. **Build API**: Ho√†n thi·ªán Inference API
5. **Create Dashboard**: T·∫°o UI cho monitoring v√† visualization

Chi ti·∫øt xem trong: **[PROJECT_ROADMAP.md](PROJECT_ROADMAP.md)**

---

## üÜò H·ªó Tr·ª£

N·∫øu g·∫∑p v·∫•n ƒë·ªÅ:

1. Ki·ªÉm tra logs: `make logs-kol`
2. Xem health status: `make health`
3. Review troubleshooting section trong **[INFRASTRUCTURE.md](INFRASTRUCTURE.md)**
4. Check Docker resources: `docker info`

---

**Ch√∫c b·∫°n th√†nh c√¥ng v·ªõi project! üéâ**
