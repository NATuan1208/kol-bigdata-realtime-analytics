# ============================================================================
# BASE PLATFORM (SME Pulse) — Shared Lakehouse + Orchestration Infrastructure
# ============================================================================
#
# This compose file defines the foundational data platform that can be reused
# across multiple projects (SME Pulse, KOL Analytics, etc.).
#
# Components:
# - MinIO (S3-compatible object storage for data lake)
# - PostgreSQL (metadata DB for Hive Metastore, Airflow, MLflow)
# - Hive Metastore (catalog for Iceberg tables)
# - Trino (distributed SQL query engine)
# - Apache Airflow (workflow orchestration)
# - dbt (SQL transformation framework)
#
# Network: All services attach to 'data-platform-net' for cross-project access
# ============================================================================

version: '3.8'

# ============================================================================
# NETWORKS
# ============================================================================
networks:
  data-platform-net:
    name: data-platform-net
    driver: bridge

# ============================================================================
# VOLUMES
# ============================================================================
volumes:
  minio_data:
    name: minio_data
  postgres_data:
    name: postgres_data
  airflow_logs:
    name: airflow_logs
  airflow_dags:
    name: airflow_dags
  airflow_plugins:
    name: airflow_plugins
  trino_data:
    name: trino_data
  hive_warehouse:
    name: hive_warehouse

# ============================================================================
# SERVICES
# ============================================================================
services:

  # ==========================================================================
  # STORAGE LAYER — MinIO (S3-compatible data lake)
  # ==========================================================================
  minio:
    image: minio/minio:RELEASE.2024-11-07T00-52-20Z
    container_name: base-minio
    hostname: minio
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minioadmin}
    ports:
      - "${MINIO_API_PORT:-9000}:9000"       # S3 API
      - "${MINIO_CONSOLE_PORT:-9001}:9001"   # Web Console
    volumes:
      - minio_data:/data
    networks:
      - data-platform-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # MinIO client for bucket initialization
  minio-init:
    image: minio/mc:RELEASE.2024-11-05T11-06-40Z
    container_name: base-minio-init
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set myminio http://minio:9000 ${MINIO_ROOT_USER:-minioadmin} ${MINIO_ROOT_PASSWORD:-minioadmin};
      mc mb --ignore-existing myminio/bronze;
      mc mb --ignore-existing myminio/silver;
      mc mb --ignore-existing myminio/gold;
      mc mb --ignore-existing myminio/mlflow;
      mc mb --ignore-existing myminio/lakehouse;
      mc mb --ignore-existing myminio/dbt;
      echo 'Buckets created successfully';
      exit 0;
      "
    networks:
      - data-platform-net

  # ==========================================================================
  # METADATA LAYER — PostgreSQL (Hive Metastore + Airflow + MLflow backend)
  # ==========================================================================
  postgres:
    image: postgres:15-alpine
    container_name: base-postgres
    hostname: postgres
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-admin}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-admin}
      POSTGRES_DB: ${POSTGRES_DB:-metastore}
      # Create multiple databases on init
      POSTGRES_MULTIPLE_DATABASES: metastore,airflow,mlflow
    ports:
      - "${POSTGRES_PORT:-5432}:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./scripts/init-postgres.sh:/docker-entrypoint-initdb.d/init-postgres.sh
    networks:
      - data-platform-net
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-admin}"]
      interval: 10s
      timeout: 5s
      retries: 5

  # ==========================================================================
  # CATALOG LAYER — Hive Metastore (Iceberg table catalog)
  # ==========================================================================
  hive-metastore:
    image: apache/hive:4.0.0
    container_name: base-hive-metastore
    hostname: hive-metastore
    environment:
      SERVICE_NAME: metastore
      DB_DRIVER: postgres
      # Connection string format: jdbc:postgresql://host:port/database
      METASTORE_DB_HOSTNAME: postgres
      METASTORE_DB_PORT: 5432
      METASTORE_DB_NAME: metastore
      METASTORE_DB_USER: ${POSTGRES_USER:-admin}
      METASTORE_DB_PASSWORD: ${POSTGRES_PASSWORD:-admin}
      # S3 configuration for MinIO
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: ${MINIO_ROOT_USER:-minioadmin}
      S3_SECRET_KEY: ${MINIO_ROOT_PASSWORD:-minioadmin}
      S3_PATH_STYLE_ACCESS: "true"
    ports:
      - "${HIVE_METASTORE_PORT:-9083}:9083"
    volumes:
      - hive_warehouse:/opt/hive/data/warehouse
    depends_on:
      postgres:
        condition: service_healthy
      minio:
        condition: service_healthy
    networks:
      - data-platform-net
    healthcheck:
      test: ["CMD", "nc", "-z", "localhost", "9083"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 60s

  # ==========================================================================
  # QUERY LAYER — Trino (distributed SQL engine)
  # ==========================================================================
  trino:
    image: trinodb/trino:435
    container_name: base-trino
    hostname: trino
    ports:
      - "${TRINO_PORT:-8080}:8080"
    volumes:
      - ./trino/etc:/etc/trino
      - trino_data:/data/trino
    environment:
      # Iceberg catalog configuration
      ICEBERG_METASTORE_URI: thrift://hive-metastore:9083
      S3_ENDPOINT: http://minio:9000
      S3_ACCESS_KEY: ${MINIO_ROOT_USER:-minioadmin}
      S3_SECRET_KEY: ${MINIO_ROOT_PASSWORD:-minioadmin}
    depends_on:
      hive-metastore:
        condition: service_healthy
      minio:
        condition: service_healthy
    networks:
      - data-platform-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/v1/info"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s

  # ==========================================================================
  # ORCHESTRATION LAYER — Apache Airflow
  # ==========================================================================
  
  # Airflow PostgreSQL initialization
  airflow-init:
    image: apache/airflow:2.8.0-python3.11
    container_name: base-airflow-init
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER:-admin}:${POSTGRES_PASSWORD:-admin}@postgres:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY:-46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=}
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      _AIRFLOW_DB_MIGRATE: "true"
      _AIRFLOW_WWW_USER_CREATE: "true"
      _AIRFLOW_WWW_USER_USERNAME: ${AIRFLOW_USERNAME:-admin}
      _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_PASSWORD:-admin}
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - data-platform-net
    command: version

  # Airflow Webserver
  airflow-webserver:
    image: apache/airflow:2.8.0-python3.11
    container_name: base-airflow-webserver
    hostname: airflow-webserver
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER:-admin}:${POSTGRES_PASSWORD:-admin}@postgres:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY:-46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
      AIRFLOW__WEBSERVER__SECRET_KEY: ${AIRFLOW_WEBSERVER_SECRET_KEY:-changeme}
    ports:
      - "${AIRFLOW_WEBSERVER_PORT:-8081}:8080"
    volumes:
      - airflow_dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
      - ../batch:/opt/airflow/batch
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    networks:
      - data-platform-net
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    command: webserver

  # Airflow Scheduler
  airflow-scheduler:
    image: apache/airflow:2.8.0-python3.11
    container_name: base-airflow-scheduler
    hostname: airflow-scheduler
    environment:
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER:-admin}:${POSTGRES_PASSWORD:-admin}@postgres:5432/airflow
      AIRFLOW__CORE__FERNET_KEY: ${AIRFLOW_FERNET_KEY:-46BKJoQYlPPOexq0OhDZnIlNepKFf87WFwLbfzqDDho=}
      AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: "true"
      AIRFLOW__CORE__LOAD_EXAMPLES: "false"
    volumes:
      - airflow_dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
      - airflow_plugins:/opt/airflow/plugins
      - ../batch:/opt/airflow/batch
    depends_on:
      airflow-init:
        condition: service_completed_successfully
      postgres:
        condition: service_healthy
    networks:
      - data-platform-net
    command: scheduler

  # ==========================================================================
  # TRANSFORMATION LAYER — dbt (SQL modeling)
  # ==========================================================================
  dbt:
    image: ghcr.io/dbt-labs/dbt-trino:1.7.0
    container_name: base-dbt
    hostname: dbt
    volumes:
      - ../dwh:/usr/app
      - ~/.dbt:/root/.dbt
    working_dir: /usr/app
    environment:
      DBT_PROFILES_DIR: /usr/app
      DBT_PROJECT_DIR: /usr/app
      # Trino connection
      TRINO_HOST: trino
      TRINO_PORT: 8080
      TRINO_USER: trino
      TRINO_CATALOG: iceberg
      TRINO_SCHEMA: silver
    depends_on:
      trino:
        condition: service_healthy
    networks:
      - data-platform-net
    command: ["tail", "-f", "/dev/null"]  # Keep container running

