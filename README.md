# ğŸŒˆ KOL Realâ€‘Time Analytics â€” Trustworthiness & Success  
### Spark Structured Streaming for Unified Processing ğŸš€

> **Má»¥c tiÃªu chiáº¿n lÆ°á»£c**  
> XÃ¢y ná»n táº£ng phÃ¢n tÃ­ch & dá»± Ä‘oÃ¡n **hiá»‡u suáº¥t KOL/Campaign theo thá»i gian thá»±c**.  
> - **Spark Structured Streaming:** xá»­ lÃ½ streaming real-time vá»›i micro-batch, exactlyâ€‘once semantics.
> - **Spark Batch:** ETL/CTAS, backfill lá»‹ch sá»­, chuáº©n hoÃ¡ Lakehouse (Iceberg) cho BI & huáº¥n luyá»‡n ML.  
> - **Unified Engine:** CÃ¹ng má»™t Spark cluster cho cáº£ streaming vÃ  batch â†’ Ä‘Æ¡n giáº£n hÆ¡n, dá»… váº­n hÃ nh.
> - Trá»ng tÃ¢m: **Eventâ€‘time**, **Exactlyâ€‘Once**, **Data Contracts**, **TÃ¡ch OLTP/OLAP**.

---

## âš ï¸ IMPORTANT: Domain Separation

**KOL Analytics** and **SME Pulse** are **TWO INDEPENDENT PROJECTS** that share the same local infrastructure instance for development efficiency. They maintain **STRICT LOGICAL SEPARATION** through:

- ğŸ—‚ï¸ **Separate MinIO buckets:** `kol-bronze`, `kol-silver`, `kol-gold` (vs `sme-*`)
- ğŸ—„ï¸ **Separate PostgreSQL databases:** `kol_mlflow`, `kol_metadata` (vs `sme_*`)
- ğŸš€ **Separate Trino schemas:** `iceberg.kol_*` (vs `iceberg.sme_*`)
- ğŸ§ª **Separate MLflow experiments:** `KOL_*` prefix (vs `SME_*`)

**ğŸ“– Read the full explanation:** [Domain Separation Architecture](docs/DOMAIN_SEPARATION.md)

This setup allows both projects to run efficiently on the same laptop while maintaining complete data and pipeline isolation. In production, each project would run on separate infrastructure.

---

## ğŸš€ Quick Start

**New to the project?** Start here:
1. ğŸ“– [**RUN_INFRASTRUCTURE.md**](RUN_INFRASTRUCTURE.md) â€” **HÆ¯á»šNG DáºªN CHáº Y Háº  Táº¦NG** (báº¯t Ä‘áº§u tá»« Ä‘Ã¢y!)
2. ğŸ“– [Quick Start Guide](QUICKSTART.md) â€” Detailed setup guide
3. ğŸ—ï¸ [Infrastructure Documentation](INFRASTRUCTURE.md) â€” Architecture and operations
4. ğŸ—ºï¸ [Project Roadmap](PROJECT_ROADMAP.md) â€” Implementation plan and next steps

**Already familiar?** Jump straight to:
```powershell
make up-kol  # Start everything
make health  # Verify all services
```

---

## ğŸ“š Documentation

| Document | Description |
|----------|-------------|
| **[DOMAIN_SEPARATION.md](docs/DOMAIN_SEPARATION.md)** | ğŸ¢ **Domain separation philosophy** - How SME & KOL share infra |
| **[RUN_INFRASTRUCTURE.md](RUN_INFRASTRUCTURE.md)** | ğŸ‡»ğŸ‡³ **HÆ¯á»šNG DáºªN CHáº Y Háº  Táº¦NG** (Vietnamese) - Start here! |
| **[QUICKSTART.md](QUICKSTART.md)** | 5-minute setup guide for first-time users |
| **[INFRASTRUCTURE.md](INFRASTRUCTURE.md)** | Complete infrastructure documentation, service details, troubleshooting |
| **[PROJECT_ROADMAP.md](PROJECT_ROADMAP.md)** | Development roadmap, implementation priorities, timelines |
| **[KOL_Architecture_Ensemble.md](KOL_Architecture_Ensemble.md)** | Detailed architecture and ML ensemble strategy |

---

## ğŸ—ï¸ Infrastructure Stack

### Base Platform (SME Pulse â€” Reusable)
- **MinIO**: S3-compatible data lake (bronze/silver/gold layers)
- **Trino**: Distributed SQL query engine for Iceberg tables
- **Hive Metastore**: Iceberg catalog backed by PostgreSQL
- **PostgreSQL**: Metadata storage (Hive, Airflow, MLflow)
- **Apache Airflow**: Workflow orchestration
- **dbt**: SQL transformation framework

### KOL Extensions
- **Redpanda**: Kafka-compatible streaming platform (simpler than Kafka+Zookeeper)
- **Apache Spark**: Unified processing engine for both streaming and batch
  - **Spark Structured Streaming**: Real-time micro-batch processing (1-10s intervals)
  - **Spark Batch**: ETL, backfill, Iceberg CTAS operations
- **MLflow**: Experiment tracking, model registry, artifact storage
- **Cassandra**: Time-series metrics storage
- **Redis**: Feature cache, pub/sub alerts
- **Trainer Service**: ML model training (Trust & Success models)
- **Inference API**: FastAPI model serving with caching

---

## ğŸŒ Access Points

Once running (`make up-kol`), access services at:

| Service | URL | Purpose |
|---------|-----|---------|
| MinIO Console | http://localhost:9001 | S3-compatible storage UI |
| Trino UI | http://localhost:8080 | SQL query interface |
| Airflow | http://localhost:8081 | Workflow management |
| Redpanda Console | http://localhost:8082 | Kafka topics & messages |
| Spark Master | http://localhost:8084 | Spark cluster & streaming jobs |
| Spark History | http://localhost:18080 | Spark job history |
| MLflow | http://localhost:5000 | Experiment tracking |
| API Docs | http://localhost:8080/docs | Interactive API documentation |
| Jupyter Lab | http://localhost:8888 | Ad-hoc analysis |

---

## âš¡ Quick Commands

```powershell
# Infrastructure
make up-kol          # Start everything (base + KOL)
make up-base         # Start only base platform
make down-all        # Stop everything
make health          # Check all services
make ps-all          # Show service status

# Logs
make logs-kol        # All KOL services
make logs-api        # API only
make logs-spark      # Spark only

# Development
make exec-api        # Shell into API container
make exec-trainer    # Shell into trainer container
make train           # Run training job
make test            # Run tests

# Utilities
make init            # Initialize env files
make clean           # Stop and remove volumes
```

Full command reference: `make help`

---

## ğŸ§­ Core Principles (NguyÃªn táº¯c cá»‘t lÃµi)

1. **â±ï¸ Eventâ€‘Time + Watermarks (Flink)**: Æ°u tiÃªn thá»i gian sá»± kiá»‡n, xá»­ lÃ½ trá»…/outâ€‘ofâ€‘order chÃ­nh xÃ¡c.  
2. **ğŸ§® Exactlyâ€‘Once E2E (Kafka â†’ Flink â†’ Iceberg)**: checkpoint, stateful ops, transactional sink.  
3. **ğŸ“œ Data Contracts**: Schema Registry + Iceberg **Schema Evolution** (thÃªm cá»™t an toÃ n, timeâ€‘travel).  
4. **ğŸ§© TÃ¡ch OLTP/OLAP**: PostgreSQL cho app/metadata/RLS; OLAP trÃªn Iceberg + Trino.  

---

## ğŸ—ºï¸ Overall Architecture (Flink Hot + Spark Cold)

```mermaid
flowchart LR
  %% === Phase 1 ===
  subgraph P1[ğŸ“¥ Ingestion & Messaging]
    direction LR
    S[ğŸ›°ï¸ Social APIs<br/>ğŸ’¼ CRM/Ads] --> KAF[(ğŸ§© Apache Kafka)]
    CDC[ğŸ”„ Debezium CDC (opt.)] --> KAF
    SR[(ğŸ“œ Schema Registry)] <--> KAF
  end

  %% === Phase 2 ===
  subgraph P2[âš™ï¸ Processing]
    direction TB

    %% HOT PATH
    subgraph HOT[ğŸ”¥ Hot Path â€” Flink (Realtime)]
      direction TB
      KAF --> FJOB[ğŸ§  Flink Jobs<br/><i>Table API/SQL â€¢ CEP â€¢ Eventâ€‘time</i>]
      FJOB --> A[ğŸ”” Alerts]
      FJOB --> FS[(âš¡ Online Feature Store<br/>Redis/PG)]
      FJOB --> MS[ğŸ¤– Model Serving API<br/>FastAPI/gRPC]
      FJOB --> ICE_S[ğŸ§Š Iceberg Sink (Silver)]
    end

    %% COLD PATH
    subgraph COLD[â„ï¸ Cold Path â€” Spark (Batch)]
      direction TB
      KAF --> SPARK[ğŸŸ  Apache Spark (Batch)<br/><i>ETL â€¢ CTAS â€¢ Backfill</i>]
      SPARK --> M[ğŸ—„ï¸ MinIO (S3)] --> I[ğŸ—‚ï¸ Apache Iceberg<br/><i>Bronze â€¢ Silver â€¢ Gold</i>]
      KAF --> KC[ğŸ”Œ Kafka Connect (opt.)] --> M
    end
  end

  %% === Phase 3 ===
  subgraph P3[ğŸ“Š Consumption & MLOps]
    direction TB
    I --> T[ğŸš€ Trino] --> BI[ğŸ“Š Metabase/Superset]
    I --> NB[ğŸ§ª Notebooks]
    I --> TRN[ğŸ‹ï¸ Batch Training (Spark/Notebooks)]
    TRN --> MR[(ğŸ“¦ Model Registry)] --> MS
    PG[(ğŸ—ƒï¸ PostgreSQL OLTP<br/><i>Metadata â€¢ RLS</i>)]
  end
```

> **Legend**: ğŸ”¥ Flink (Hot) â€¢ â„ï¸ Spark (Cold) â€¢ ğŸ§© Kafka â€¢ ğŸ§  Flink â€¢ ğŸŸ  Spark â€¢ ğŸ§Š Iceberg â€¢ ğŸ—„ï¸ MinIO â€¢ ğŸš€ Trino â€¢ ğŸ“Š BI â€¢ ğŸ—ƒï¸ PostgreSQL â€¢ ğŸ¤– Model Serving â€¢ âš¡ Feature Store â€¢ ğŸ“œ Contracts

---

## ğŸ”¥ Hot Path â€” Flink (Realtime, msâ€“s)

```mermaid
flowchart TD
  A[ğŸ§© Kafka Source] --> W{â±ï¸ Watermarks<br/><i>Eventâ€‘time</i>}
  W --> S1[ğŸ§ª Stateless]
  S1 --> ST{ğŸ§  Stateful}
  ST -- Windows --> MET[ğŸ“ˆ KPIs Windowed]
  ST -- CEP --> CEP[ğŸ§­ Pattern Detection]

  subgraph ENR[ğŸ§© Enrichment & Scoring]
    L[âš¡ Lookup Features (Redis/PG)] --> CALL[ğŸ¤– Model API]
    CALL --> SC[âœ… Trust/Success Score]
  end

  MET --> L
  CEP --> L

  SC --> OUT{ğŸšš Sinks}
  OUT -- ğŸ”” --> AL[Alerts]
  OUT -- âš¡ --> FSW[Feature Writeâ€‘back]
  OUT -- ğŸ§Š --> ICE[Iceberg (Silver)]
```

**Key notes**
- **Exactlyâ€‘once** (Kafkaâ†’Flinkâ†’Iceberg), checkpoint 30â€“60s, savepoint trÆ°á»›c deploy.  
- **Latency má»¥c tiÃªu**: ingest < **1s**, alert < **30s**.  

---

## â„ï¸ Cold Path â€” Spark (Batch, minâ€“hours)

```mermaid
flowchart LR
  KR[ğŸ§© Kafka (Replay)] --> SP[ğŸŸ  Spark Batch<br/><i>ETL â€¢ CTAS â€¢ Backfill</i>]
  SP --> M[ğŸ—„ï¸ MinIO (S3)]
  M --> I[ğŸ—‚ï¸ Apache Iceberg<br/><i>Bronze â€¢ Silver â€¢ Gold</i>]
  I --> T[ğŸš€ Trino] --> BI[ğŸ“Š BI Dashboards]
```

**Best practices**
- **Partition**: `dt`, `creator_id`/`campaign_id`; **compact** file nhá»; **snapshot cleanup**.  
- **Spark** Ä‘áº£m nhiá»‡m: chuáº©n hoÃ¡, dedupe, aggregate; **CTAS** Ä‘á»ƒ sinh báº£ng Gold phá»¥c vá»¥ BI.  

---

## ğŸ§° Tooling & Vai trÃ² theo giai Ä‘oáº¡n

| Háº¡ng má»¥c | CÃ´ng cá»¥ | Vai trÃ² & LÃ½ do chá»n |
|---|---|---|
| Ingestion | **Kafka**, **Debezium** | Bus chá»‹u táº£i cao, replay; CDC tá»« DB/CRM náº¿u cáº§n |
| Contracts | **Schema Registry** | Buá»™c schema, thay Ä‘á»•i **backward-compatible** |
| Hot Path | **Flink** | Eventâ€‘time, stateful windows, **CEP**, exactlyâ€‘once |
| Cold Path | **Spark (Batch)** | ETL/CTAS, backfill, optimize/compact Iceberg |
| Lakehouse | **MinIO + Iceberg** | LÆ°u trá»¯ + ACID table format, schema evolution |
| Query | **Trino** | SQL trÃªn Iceberg cho BI/Adâ€‘hoc/Exploration |
| OLTP | **PostgreSQL (RLS)** | Metadata, config, multiâ€‘tenant cho backend |
| Features | **Redis/PG** | Online/Offline feature store |
| Serving | **FastAPI/gRPC** | Model microservice, A/B test, circuit breaker |
| BI | **Metabase/Superset** | Dashboard KPI, cohort/attribution |
| Obs. | **Prometheus/Grafana** | Kafka lag, Flink metrics, API SLIs |

---

## ğŸ¯ Má»¥c tiÃªu & KPIs

- **Realtime**: ingest < **1s**, alert < **30s**, dashboard < **1m**.  
- **TÃ­nh toÃ n váº¹n**: exactlyâ€‘once; khÃ´ng doubleâ€‘count.  
- **BI**: truy váº¥n KPI 30 ngÃ y < **10s** qua Trino.  
- **Ops**: savepoint trÆ°á»›c deploy; rollback < **5 phÃºt**.  

---

## ğŸ§ª ML & Feedback Loop

- **Training (Batch/Spark/Notebooks)** Ä‘á»c Iceberg (Silver/Gold); log & registry; deploy ra **Model API**.  
- **Realtime scoring** gá»i tá»« Flink; writeâ€‘back features & inference logs vÃ o Iceberg â†’ **vÃ²ng láº·p há»c**.  

---

## âœ… Checklist triá»ƒn khai

- [ ] Kafka topics cÃ³ key (`creator_id`/`brand_id`) + Schema Registry active.  
- [ ] Flink jobs báº­t checkpoint + exactlyâ€‘once; savepoint trÆ°á»›c deploy.  
- [ ] Spark ETL cÃ³ **CTAS**/optimize + compact Ä‘á»‹nh ká»³; Iceberg partition há»£p lÃ½.  
- [ ] Trino catalog hoáº¡t Ä‘á»™ng; BI dashboards render ngon.  
- [ ] Observability: Grafana hiá»ƒn thá»‹ lag/checkpoint/API error; alert khÃ´ng spam.  

---

## ğŸ” Báº£o máº­t

- **RLS** trÃªn PostgreSQL; **IAM**/policy MinIO; secrets qua Vault/SSM; tÃ¡ch **prod/staging**.  

> **TL;DR**: **Flink** náº¯m luá»“ng **nÃ³ng** Ä‘á»ƒ quyáº¿t Ä‘á»‹nh **tá»©c thÃ¬**; **Spark** xá»­ lÃ½ **láº¡nh** Ä‘á»ƒ chuáº©n hoÃ¡ dá»¯ liá»‡u cho **BI & training** â€” táº¥t cáº£ dá»±a trÃªn **Kafka + Iceberg + Trino** vá»›i contracts & quan sÃ¡t cháº·t cháº½.
